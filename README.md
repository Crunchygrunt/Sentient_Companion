# Sentient Companion - A Multimodal AI Assistant

## Overview

**MultiSense Companion** is an innovative project that combines facial expression recognition, voice activation, and a virtual assistant to create a personalized and emotionally responsive AI companion. The project leverages deep learning techniques to recognize users' emotions through facial expressions, enhancing the interaction between users and their virtual assistant. The assistant responds dynamically, taking into account the user's emotional state, thereby providing a more human-like and empathetic experience.

## Key Features

1. **Facial Expression Recognition (FER):**
   - Utilizes a Convolutional Neural Network (CNN) model trained on a diverse dataset of facial expressions.
   - Real-time emotion detection through webcam integration.
   - Recognizes emotions such as happiness, sadness, anger, surprise, fear, disgust, and neutrality.

2. **Voice-Activated Assistant:**
   - Enables hands-free interaction with the virtual assistant using voice commands.
   - Implements a voice recognition system to interpret user commands and respond accordingly.

3. **Dynamic Responses Based on Emotion:**
   - Tailors responses of the virtual assistant based on the user's detected emotional state.
   - Enhances user engagement by adapting to different emotional contexts.

4. **Music Playback Integration:**
   - Allows users to request and play music using voice commands.
   - Utilizes Spotify API for online music streaming.

5. **Email Interaction:**
   - Sends emails based on user commands, integrating email functionality seamlessly.

6. **Code Execution:**
   - Opens coding environments, such as Visual Studio Code, on user command.

## Real-Life Applications

- **Mental Health Support:**
  - Provides emotional support by recognizing and responding to users' emotions, offering uplifting content when needed.

- **Personal Well-Being:**
  - Encourages positive experiences through personalized responses and music playback.

- **Human-Computer Interaction:**
  - Showcases the potential of emotional intelligence in improving the interaction between humans and AI.

## Project Setup

### Requirements

- Python 3.x
- TensorFlow
- Keras
- OpenCV
- Pyttsx3
- SpeechRecognition
- Wikipedia
- PyAudio
- Spotipy (Spotify API)

### Steps to Run

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/MultiSense-Companion.git
   
2. Navigate to the project directory
   
    ```bash
   cd MultiSense-Companion
    
4. Install Dependencies
   
   ```bash
   pip install -r requirements.txt
   
5. Run the main assistant script
   ```bash
   python assistant.py

Interact with the MultiSense Companion using voice commands and experience the dynamic responses based on facial expressions.

   



